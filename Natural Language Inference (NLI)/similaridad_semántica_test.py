# -*- coding: utf-8 -*-
"""Similaridad sem√°ntica_Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fTr7rJd4HlfAbcmDg_7Cnr_8OULY0zPB

# Librerias
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets spacy fasttext transformers seqeval ipdb
# !python -m spacy download es

from torch.nn.utils.rnn import pad_sequence

"""# Dataset"""

from datasets import load_dataset
dataset = load_dataset("stsb_multi_mt", "es")
dataset

dataset['test'][0]

"""#Similaridad Coseno"""

import numpy as np

def cossim(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

"""# Bert

## Dataset
"""

from datasets import load_dataset
dataset = load_dataset("stsb_multi_mt", "es")
dataset

"""## Modelo y tokenizador"""

from transformers import AutoTokenizer, AutoModel

model_name = "dccuchile/bert-base-spanish-wwm-cased"  
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenizer.tokenize(dataset['test'][0]['sentence1'])

tokenizer(dataset['test'][0]['sentence1'])

def tokenize(example):
  sen1_tok = tokenizer(example['sentence1'], truncation=True)
  sen2_tok = tokenizer(example['sentence2'], truncation=True)
  ret = {}

  for key, value in sen1_tok.items():
    new_key= 's1_'+key
    ret[new_key] = value

  for key, value in sen2_tok.items():
    new_key= 's2_'+key
    ret[new_key] = value
    
  return ret

dataset = dataset.map(tokenize, batched=True, remove_columns=['sentence1','sentence2'])

dataset['test'][0]

"""##Dataloader"""

from torch.utils.data import DataLoader
import torch

def collate_batch(batch):
    
    new_batch = {}
    for example in batch:
        for k, v in example.items():
            if k not in new_batch:
                new_batch[k] = []
            new_batch[k].append(v)
    
    batch = new_batch
    similarity_score = batch.pop("similarity_score")

    sentence_1 = {
      k.replace("s1_", ""):v for 
      k, v in batch.items() if k.startswith("s1_")}
    sentence_2 = {
        k.replace("s2_", ""):v for 
        k, v in batch.items() if k.startswith("s2_")}
    sentence_1 = tokenizer.pad(sentence_1)
    sentence_2 = tokenizer.pad(sentence_2)
    

    inputs_s1 = {k:torch.LongTensor(v) for k, v in sentence_1.items()}
    inputs_s2 = {k:torch.LongTensor(v) for k, v in sentence_2.items()}
    similarity_score = torch.LongTensor(similarity_score)

    return inputs_s1, inputs_s2, similarity_score

train_dataloader = DataLoader(dataset["train"], batch_size=32, collate_fn=collate_batch)
val_dataloader = DataLoader(dataset["dev"], batch_size=16, collate_fn=collate_batch)
test_dataloader = DataLoader(dataset["test"], batch_size=16, collate_fn=collate_batch)

inputs_s1, inputs_s2, similarity_score = next(iter(test_dataloader))

inputs_s1.keys()
inputs_s2.keys()

"""##Modelo"""

base_model = AutoModel.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
device = "cpu"
base_model.to(device)

cossim_out=[]
 for batch in test_dataloader:
        inputs_s1, inputs_s2, similarity_score = batch
        if device:
            inputs_s1 = {k:v.to(device) for k, v in inputs_s1.items()}
            inputs_s2 = {k:v.to(device) for k, v in inputs_s2.items()}

        ret_s1 = base_model(**inputs_s1)
        ret_s2 = base_model(**inputs_s2)

        s1 = ret_s1.pooler_output.detach()
        s2 = ret_s2.pooler_output.detach()

        for i in range(len(s1)):
          dist = cossim(s1[i],s2[i])
          cossim_out.append(dist)

"""## Correlacion

"""

labels = dataset['test']['similarity_score']

"""### Pearson"""

corr = np.corrcoef(labels,cossim_out)

corr

from scipy.stats.stats import pearsonr 
print(pearsonr(labels,cossim_out))

"""### Spearman"""

from scipy import stats
stats.spearmanr(labels,cossim_out)

"""# Roberta

## Dataset
"""

from datasets import load_dataset
dataset = load_dataset("stsb_multi_mt", "es")
dataset

"""##Modelo y tokenizador"""

from transformers import AutoTokenizer, AutoModel
model_name = "mrm8488/RuPERTa-base"  
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
  sen1_tok = tokenizer(example['sentence1'], truncation=True)
  sen2_tok = tokenizer(example['sentence2'], truncation=True)
  ret = {}

  for key, value in sen1_tok.items():
    new_key= 's1_'+key
    ret[new_key] = value

  for key, value in sen2_tok.items():
    new_key= 's2_'+key
    ret[new_key] = value
    
  return ret

dataset = dataset.map(tokenize, batched=True, remove_columns=['sentence1','sentence2'])

dataset['test'][0]

"""## Dataloader"""

from torch.utils.data import DataLoader
import torch

def collate_batch(batch):
    
    new_batch = {}
    for example in batch:
        for k, v in example.items():
            if k not in new_batch:
                new_batch[k] = []
            new_batch[k].append(v)
    
    batch = new_batch
    similarity_score = batch.pop("similarity_score")

    sentence_1 = {
      k.replace("s1_", ""):v for 
      k, v in batch.items() if k.startswith("s1_")}
    sentence_2 = {
        k.replace("s2_", ""):v for 
        k, v in batch.items() if k.startswith("s2_")}
    sentence_1 = tokenizer.pad(sentence_1)
    sentence_2 = tokenizer.pad(sentence_2)
    

    inputs_s1 = {k:torch.LongTensor(v) for k, v in sentence_1.items()}
    inputs_s2 = {k:torch.LongTensor(v) for k, v in sentence_2.items()}
    similarity_score = torch.LongTensor(similarity_score)

    return inputs_s1, inputs_s2, similarity_score

train_dataloader = DataLoader(dataset["train"], batch_size=32, collate_fn=collate_batch)
val_dataloader = DataLoader(dataset["dev"], batch_size=16, collate_fn=collate_batch)
test_dataloader = DataLoader(dataset["test"], batch_size=16, collate_fn=collate_batch)

inputs_s1, inputs_s2, similarity_score = next(iter(test_dataloader))

inputs_s1.keys()
inputs_s2.keys()

"""## Modelo"""

base_model = AutoModel.from_pretrained("mrm8488/RuPERTa-base")
device = "cpu"
base_model.to(device)

cossim_out=[]
for batch in test_dataloader:
        inputs_s1, inputs_s2, similarity_score = batch
        if device:
            inputs_s1 = {k:v.to(device) for k, v in inputs_s1.items()}
            inputs_s2 = {k:v.to(device) for k, v in inputs_s2.items()}

        ret_s1 = base_model(**inputs_s1)
        ret_s2 = base_model(**inputs_s2)

        s1 = ret_s1.pooler_output.detach()
        s2 = ret_s2.pooler_output.detach()

        for i in range(len(s1)):
          dist = cossim(s1[i],s2[i])
          cossim_out.append(dist)

len(cossim_out)

"""## Correlacion

"""

labels = dataset['test']['similarity_score']

len(labels)

"""### Pearson"""

corr = np.corrcoef(labels,cossim_out)

corr

from scipy.stats.stats import pearsonr 
print(pearsonr(labels,cossim_out))

"""### Spearman"""

from scipy import stats
stats.spearmanr(labels,cossim_out)

"""# modelo_NLI_kvd_vf_5000

## Dataset
"""

from datasets import load_dataset
dataset = load_dataset("stsb_multi_mt", "es")
dataset

"""##Modelo y tokenizador"""

from transformers import AutoTokenizer, AutoModel
model_name = "florver/modelo_NLI_kvd_vf_5000_v2"  
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
  sen1_tok = tokenizer(example['sentence1'], truncation=True)
  sen2_tok = tokenizer(example['sentence2'], truncation=True)
  ret = {}

  for key, value in sen1_tok.items():
    new_key= 's1_'+key
    ret[new_key] = value

  for key, value in sen2_tok.items():
    new_key= 's2_'+key
    ret[new_key] = value
    
  return ret

dataset = dataset.map(tokenize, batched=True, remove_columns=['sentence1','sentence2'])

"""##Dataloader"""

from torch.utils.data import DataLoader
import torch

def collate_batch(batch):
    
    new_batch = {}
    for example in batch:
        for k, v in example.items():
            if k not in new_batch:
                new_batch[k] = []
            new_batch[k].append(v)
    
    batch = new_batch
    similarity_score = batch.pop("similarity_score")

    sentence_1 = {
      k.replace("s1_", ""):v for 
      k, v in batch.items() if k.startswith("s1_")}
    sentence_2 = {
        k.replace("s2_", ""):v for 
        k, v in batch.items() if k.startswith("s2_")}
    sentence_1 = tokenizer.pad(sentence_1)
    sentence_2 = tokenizer.pad(sentence_2)
    

    inputs_s1 = {k:torch.LongTensor(v) for k, v in sentence_1.items()}
    inputs_s2 = {k:torch.LongTensor(v) for k, v in sentence_2.items()}
    similarity_score = torch.LongTensor(similarity_score)

    return inputs_s1, inputs_s2, similarity_score

train_dataloader = DataLoader(dataset["train"], batch_size=32, collate_fn=collate_batch)
val_dataloader = DataLoader(dataset["dev"], batch_size=16, collate_fn=collate_batch)
test_dataloader = DataLoader(dataset["test"], batch_size=16, collate_fn=collate_batch)

inputs_s1, inputs_s2, similarity_score = next(iter(test_dataloader))

inputs_s1.keys()
inputs_s2.keys()

"""##Modelo"""

base_model = AutoModel.from_pretrained("florver/modelo_NLI_kvd_vf_5000_v2")
device = "cpu"
base_model.to(device)

cossim_out=[]
for batch in test_dataloader:
        inputs_s1, inputs_s2, similarity_score = batch
        if device:
            inputs_s1 = {k:v.to(device) for k, v in inputs_s1.items()}
            inputs_s2 = {k:v.to(device) for k, v in inputs_s2.items()}

        ret_s1 = base_model(**inputs_s1)
        ret_s2 = base_model(**inputs_s2)


        s1 = ret_s1.pooler_output.detach()
        s2 = ret_s2.pooler_output.detach()

        for i in range(len(s1)):
          dist = cossim(s1[i],s2[i])
          cossim_out.append(dist)

len(cossim_out)

"""## Correlacion"""

labels = dataset['test']['similarity_score']

len(labels)

"""### Pearson"""

corr = np.corrcoef(labels,cossim_out)
corr

"""### Spearman"""

from scipy import stats
stats.spearmanr(labels,cossim_out)

"""# modelo_NLI_kvd_2_5000

## Dataset
"""

from datasets import load_dataset
dataset = load_dataset("stsb_multi_mt", "es")
dataset

"""##Modelo y tokenizador"""

from transformers import AutoTokenizer, AutoModel
model_name = "florver/modelo_NLI_kvd_2_5000"  
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
  sen1_tok = tokenizer(example['sentence1'], truncation=True)
  sen2_tok = tokenizer(example['sentence2'], truncation=True)
  ret = {}

  for key, value in sen1_tok.items():
    new_key= 's1_'+key
    ret[new_key] = value

  for key, value in sen2_tok.items():
    new_key= 's2_'+key
    ret[new_key] = value
    
  return ret

dataset = dataset.map(tokenize, batched=True, remove_columns=['sentence1','sentence2'])

"""##Dataloader"""

from torch.utils.data import DataLoader
import torch

def collate_batch(batch):
    
    new_batch = {}
    for example in batch:
        for k, v in example.items():
            if k not in new_batch:
                new_batch[k] = []
            new_batch[k].append(v)
    
    batch = new_batch
    similarity_score = batch.pop("similarity_score")

    sentence_1 = {
      k.replace("s1_", ""):v for 
      k, v in batch.items() if k.startswith("s1_")}
    sentence_2 = {
        k.replace("s2_", ""):v for 
        k, v in batch.items() if k.startswith("s2_")}
    sentence_1 = tokenizer.pad(sentence_1)
    sentence_2 = tokenizer.pad(sentence_2)
    

    inputs_s1 = {k:torch.LongTensor(v) for k, v in sentence_1.items()}
    inputs_s2 = {k:torch.LongTensor(v) for k, v in sentence_2.items()}
    similarity_score = torch.LongTensor(similarity_score)

    return inputs_s1, inputs_s2, similarity_score

train_dataloader = DataLoader(dataset["train"], batch_size=32, collate_fn=collate_batch)
val_dataloader = DataLoader(dataset["dev"], batch_size=16, collate_fn=collate_batch)
test_dataloader = DataLoader(dataset["test"], batch_size=16, collate_fn=collate_batch)

inputs_s1, inputs_s2, similarity_score = next(iter(test_dataloader))

inputs_s1.keys()
inputs_s2.keys()

"""##Modelo"""

base_model = AutoModel.from_pretrained("florver/modelo_NLI_kvd_2_5000")
device = "cpu"
base_model.to(device)

cossim_out=[]
for batch in test_dataloader:
        inputs_s1, inputs_s2, similarity_score = batch
        if device:
            inputs_s1 = {k:v.to(device) for k, v in inputs_s1.items()}
            inputs_s2 = {k:v.to(device) for k, v in inputs_s2.items()}

        ret_s1 = base_model(**inputs_s1)
        ret_s2 = base_model(**inputs_s2)


        s1 = ret_s1.pooler_output.detach()
        s2 = ret_s2.pooler_output.detach()

        for i in range(len(s1)):
          dist = cossim(s1[i],s2[i])
          cossim_out.append(dist)

len(cossim_out)

"""## Correlacion"""

labels = dataset['test']['similarity_score']

"""### Pearson"""

corr = np.corrcoef(labels,cossim_out)
corr

"""### Spearman"""

from scipy import stats
stats.spearmanr(labels,cossim_out)