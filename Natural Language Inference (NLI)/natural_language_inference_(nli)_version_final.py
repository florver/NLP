# -*- coding: utf-8 -*-
"""Natural Language Inference (NLI) - Version Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kUNgi-HLnkvDrS48VwU1l6Q3NFLwUsEk

## Librerias
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets spacy fasttext transformers seqeval ipdb
# !python -m spacy download es
#

"""## Dataset

Cargo el dataset de Hugging Face que utilizamos para entrenar NLI.
"""

from datasets import load_dataset
dataset = load_dataset("xnli", "es")
dataset

dataset['train'][0]

"""Clases posibles: 
0 (entailment)
1 (neutral)
2 (contradiction)

## Modelo y tokenizador
"""

from transformers import AutoTokenizer, AutoModel

model_name = "dccuchile/bert-base-spanish-wwm-cased"  
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenizer.tokenize(dataset['train'][0]['hypothesis'])

tokenizer(dataset['train'][0]['hypothesis'])

tokenizer.model_max_length = 128

"""### Tokenizador

"""

def tokenize(example):
  hip_tok = tokenizer(example['hypothesis'], truncation=True)
  prem_tok = tokenizer(example['premise'], truncation=True)
  ret = {}

  for key, value in hip_tok.items():
    new_key= 'h_'+key
    ret[new_key] = value

  for key, value in prem_tok.items():
    new_key= 'p_'+key
    ret[new_key] = value
    
  return ret

dataset = dataset.map(tokenize, batched=True, remove_columns=['hypothesis','premise'])

dataset['train'][0]

id2label = dataset["train"].features["label"].names

label2id = {v:k for k, v in enumerate(id2label)}

id2label

"""# Dataloader"""

from torch.utils.data import DataLoader
import torch

def collate_batch(batch):
    

    new_batch = {}
    for example in batch:
        for k, v in example.items():
            if k not in new_batch:
                new_batch[k] = []
            new_batch[k].append(v)
    
    batch = new_batch

    labels = batch.pop("label")

    hypothesis = {
      k.replace("h_", ""):v for 
      k, v in batch.items() if k.startswith("h_")}
    premise = {
        k.replace("p_", ""):v for 
        k, v in batch.items() if k.startswith("p_")}
    hypothesis = tokenizer.pad(hypothesis)
    premise = tokenizer.pad(premise)   

    inputs_h = {k:torch.LongTensor(v) for k, v in hypothesis.items()}
    inputs_p = {k:torch.LongTensor(v) for k, v in premise.items()}
    labels = torch.LongTensor(labels)

    return inputs_h, inputs_p, labels

train_dataloader = DataLoader(dataset["train"], batch_size=32, collate_fn=collate_batch)
val_dataloader = DataLoader(dataset["validation"], batch_size=16, collate_fn=collate_batch)
test_dataloader = DataLoader(dataset["test"], batch_size=16, collate_fn=collate_batch)

inputs_h, inputs_p, labels = next(iter(train_dataloader))

inputs_h.keys()
inputs_p.keys()

"""# Modelo Pre-entrenado"""

from transformers import AutoModel

base_model = AutoModel.from_pretrained(model_name)

"""# Modelo"""

from torch import nn

class NLI_Hipotesis(nn.Module):
    def __init__(self, base_model, dropout=0.1):
        super().__init__()
        self.base_model = base_model
        self.fc = nn.Linear(768*3, 3) 
        self.dropout = nn.Dropout(dropout)

    def forward(self, inputs_h, inputs_p):   
        out_h = self.base_model(**inputs_h)
        u = out_h.pooler_output
        out_p = self.base_model(**inputs_p)
        v = out_p.pooler_output
        u_v = torch.abs(u - v)
        concat = torch.cat((u, v, u_v),dim = 1)  
        pooler = self.dropout(concat)
        return self.fc(pooler)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir /content/runs

import torch
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()

def log_tensorboard(tag, value, step):
    writer.add_scalar(tag, value, global_step=step)

from torch.nn import functional as F
from sklearn.metrics import classification_report

def compute_loss(logits, labels):
    """
    Computa la pérdida
    """
    return F.cross_entropy(
        logits, 
        labels)
        
def validate(model, dataloader, device=None):
    sum_loss = .0
    num_batches = 0

    labels = []
    outputs = []
    for batch in val_dataloader:
        inputs_h,inputs_p, label = batch
        if device:
            inputs_h = {k:v.to(device) for k, v in inputs_h.items()}
            inputs_p = {k:v.to(device) for k, v in inputs_p.items()}
            label = label.to(device)

        num_batches += 1
        outs = model.forward(inputs_h,inputs_p)
        
        sum_loss += compute_loss(outs, label).detach().cpu()
        labels.append(label.detach())
        outputs.append(outs.detach())

    preds = torch.cat(outputs).argmax(1).cpu().numpy()
    gold = torch.cat(labels).cpu().numpy()


    ret = classification_report(gold, preds, output_dict=True)
   
    for k in [0, 1, 2]:
        ret[id2label[k]] = ret[str(k)]
        del ret[str(k)]
    loss = sum_loss / num_batches
    ret["loss"] = loss.item()
    return ret


def val_check(model, dataloader, device, step):
    dev_results = validate(model, dataloader, device)
    for key in ['entailment', 'neutral', 'contradiction']:
        log_tensorboard(key.capitalize()+ " Macro F1", dev_results[key]["f1-score"], step=step)
    log_tensorboard("Dev Macro F1", dev_results['macro avg']["f1-score"], step=step)
    log_tensorboard("Dev Loss", dev_results["loss"], step=step)
    log_tensorboard("Accuracy", dev_results["accuracy"], step=step)

from tqdm.auto import tqdm
from transformers import get_scheduler, AdamW
device = "cuda"

model = NLI_Hipotesis(base_model)
model = model.to(device)

lr = 5e-5  
optimizer = AdamW(model.parameters(), lr=lr)


num_training_steps = 2_000 
scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

device = "cuda"

stop = False
current_step = 0

val_check_steps = 5_000

pbar = tqdm(total=num_training_steps)

val_check(model, val_dataloader, device, current_step)


while not stop:
    for batch in train_dataloader:
        inputs_h, inputs_p, label = batch
        if device:
            inputs_h = {k:v.to(device) for k, v in inputs_h.items()}
            inputs_p = {k:v.to(device) for k, v in inputs_p.items()}
            label = label.to(device)
        optimizer.zero_grad()
        
        outs = model(inputs_h,inputs_p)
        loss = compute_loss(outs, label)
        loss.backward()

        optimizer.step()
        scheduler.step()

                
        current_lr = scheduler.get_last_lr()[0]
        pbar.update()

        log_tensorboard('train_loss', loss, step=current_step)        
        log_tensorboard('lr', current_lr, step=current_step)
        desc = f"Epoch  -- Train loss {loss:.3f}"
        pbar.set_description(desc)
        current_step += 1

        if current_step > val_check_steps:
          stop = True
          break

        if current_step % num_training_steps == 0: 
          val_check(model, val_dataloader, device, current_step)

#Test
validate(model, test_dataloader, device="cuda")

#Train
validate(model, train_dataloader, device="cuda")

#Validación
validate(model, val_dataloader, device="cuda")

"""# Hugging face"""

from huggingface_hub import notebook_login

notebook_login()

#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

base_model.push_to_hub("modelo_NLI_kvd_vf_5000_v2")
tokenizer.push_to_hub("modelo_NLI_kvd_vf_5000_v2")

kvd_1_1epoch_model = AutoModel.from_pretrained("florver/modelo_NLI_kvd_vf_5000_v2")
tokenizer = AutoTokenizer.from_pretrained("florver/modelo_NLI_kvd_vf_5000_v2")